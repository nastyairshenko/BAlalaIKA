1) Python 3.11, локально:
python -m venv .venv && .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
copy .env.example .env  # заполните TG_BOT_TOKEN и LLM_API_KEY
python -m src.bot.tg_bot - запуск бота 

2) Команды:
 /start /help /topic <тема> /roleplay /cancel

3) Архитектура:
- VAD (Valence по тексту) -> Moral Schemas -> Action Preset -> LLM (DeepSeek).
- Схемы: mentorship, comradeship, social_empathy, professional_help + 6 дидактических (warmup, gentle correction, roleplay, celebrate, flow switch, micro-goal).



Теоретическая база

eBICA — эмоциональное расширение когнитивной архитектуры GMU-BICA: добавляются (1) семантическая карта эмоциональных значений (обычно измерения Valence–Arousal–Dominance), (2) оценка-приложение (appraisal) для каждого представления, (3) моральные схемы — особый класс схем, работающих с этими значениями. Идея — объединить «рациональную» и «эмоциональную» обработку в одной модели для правдоподобного социального поведения ИИ.

Слабая семантическая карта (weak semantic map): низкоразмерное пространство (обычно V–A–D), где координаты имеют понятный смысл; карта строится эмпирически (опросы/редукция размерности) и используется как общая «сцена» для эмоций/оценок.

Моральные схемы как «агенты» с нормой: каждая моральная схема стремится поддерживать «нормальное» состояние (ценность, стандарт), влияя на выбор действий через bias — смещение вероятностей выбора. Эмоциональные флюенты (A,F,S,E,R,M,B) описывают состояние: Mood — текущее состояние активной схемы; Bias — поведенческие смещения; Reaction — невольные эмоциональные проявления и т. п.

Зачем это нужно: одной VAD-проекции недостаточно для сложных социальных эмоций; добавление моральных схем «надстраивает» нормы и связи, делая поведение убедимым в диалогах человек–ИИ.

Как это отражено в твоём боте (что уже сделано)

Извлечение Valence из текста → VAD: в tg_bot.py используется VADER (compound) и функция fuse(...) из emotion_fusion.py, чтобы получить вектор VAD. Это — прикладной аналог «оценки-приложения» на слабой семантической карте.

Моральные схемы и выбор действия: модули moral_schemas.py → schema_manager.py → policy_router.py задают «нормы» тьютора (дружелюбие, мягкая коррекция, ролеплей, поощрение и др.) и маршрутизируют диалоговые actions в зависимости от темы, хода урока и VAD. Это и есть еBICA-механизм: активная схема задаёт целевое «нормальное» состояние и смещает выбор последующего действия (bias).

Формирование ответа: virtual_tutor.py собирает системную инструкцию (с учётом темы /topic, флагов warmup/roleplay) и обращается к LLM через deepseek_interface.py. Таким образом «рациональная» генерация опирается на «эмоциональные» условия и схему — как требует eBICA.

Телеграм-слой: tg_bot.py ведёт сценарий: спрашивает тему, принимает её «свободным текстом», делает warm-up вопросы (action contextual_smalltalk), затем поддерживает разговор с мягкой коррекцией и вопросами-затравками. Это соответствует социальной цели eBICA — правдоподобное, человекоподобное взаимодействие.

Почему это «правдоподобно» по eBICA

Семантическая карта (VAD) обеспечивает общий язык для чувств/оценок;

Моральные схемы придают норму и цель («быть тёплым, поддерживающим наставником»), а также смещают вероятность диалоговых действий (коррекция, ролеплей и т. п.) — это тот самый Bias.

На каждом шаге диалога итоговый ответ — это связка «оценка (VAD) → активная схема → выбранное действие → формулировка LLM», что прямо следует из расширения eBICA для социально-эмоционального ИИ.